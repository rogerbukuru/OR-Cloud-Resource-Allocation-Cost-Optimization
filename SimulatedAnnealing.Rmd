---
title: "SimulatedAnnealing"
author: "Roger Bukuru"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Load necessary libraries
library(tidyverse)

# Clear workspace
rm(list = ls())

# Load data
fileDir <- "vm_cloud_data.csv"
vm_cloud <- read.csv(fileDir)

# Select relevant columns (skip the first two columns as mentioned)
vm_cloud <- vm_cloud[1:11000, -c(1,2)]

# Step 1: One-hot encode character features
# Identify character columns
char_cols <- vm_cloud %>%
  select(where(is.character))

# One-hot encode using pivot_wider
vm_cloud_encoded <- vm_cloud %>%
  mutate(across(where(is.character), as.factor)) %>%
  model.matrix(~ . - 1, data = .) %>%
  as.data.frame()

# Step 2: Remove rows with null values
vm_cloud_clean <- vm_cloud_encoded %>%
  drop_na()

# Step 3: Min-Max Normalize numeric features
min_max_normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# Apply min-max normalization to numeric columns
vm_cloud_standardized <- vm_cloud_clean %>%
  mutate(across(where(is.numeric), ~ min_max_normalize(.)))

# View the first few rows of the cleaned and normalized data
head(vm_cloud_standardized, 10)


# Select relevant columns for the optimization problem (after one-hot encoding and standardization)
vm_cloud_clean_analysis <- vm_cloud_standardized %>%
  select(cpu_usage, memory_usage, network_traffic, power_consumption,
         num_executed_instructions, execution_time, energy_efficiency, task_priorityhigh, task_prioritylow, task_prioritymedium)

# Preview the cleaned data
head(vm_cloud_clean_analysis)

```


```{r}
# Load necessary libraries
library(tidyverse)

# Enhanced Objective Function (unchanged)
objective_function <- function(solution, energy_efficiency, execution_time, 
                              cpu_usage, memory_usage, network_traffic, power_consumption, 
                              max_cpu, max_memory, max_net, max_pwr, penalty_weight) {
  # Calculate total execution time and energy efficiency
  total_execution_time <- sum(execution_time * solution)
  total_energy_efficiency <- sum(energy_efficiency * solution)
  
  # Calculate resource usage per server
  servers <- unique(solution)
  resource_overuse <- 0
  for (j in servers) {
    # Identify tasks assigned to server j
    tasks_j <- which(solution == j)
    
    # Sum resources used by these tasks
    total_cpu_j <- sum(cpu_usage[tasks_j])
    total_memory_j <- sum(memory_usage[tasks_j])
    total_net_j <- sum(network_traffic[tasks_j])
    total_pwr_j <- sum(power_consumption[tasks_j])
    
    # Calculate overuse for each resource
    over_cpu <- max(0, total_cpu_j - max_cpu)
    over_mem <- max(0, total_memory_j - max_memory)
    over_net <- max(0, total_net_j - max_net)
    over_pwr <- max(0, total_pwr_j - max_pwr)
    
    # Accumulate total overuse with penalties
    resource_overuse <- resource_overuse + (over_cpu + over_mem + over_net + over_pwr) * penalty_weight
  }
  
  # Total Objective
  # Minimize Execution Time and maximize Energy Efficiency
  # Implemented as: alpha * Execution_Time - beta * Energy_Efficiency + Penalties
  alpha <- 1  # Weight for Execution Time
  beta <- 1   # Weight for Energy Efficiency
  
  total_objective <- alpha * total_execution_time - beta * total_energy_efficiency + resource_overuse
  
  return(total_objective)
}

# Generate a random neighbor by reassigning a random task to a different server (unchanged)
generate_neighbor <- function(solution, num_servers) {
  neighbor <- solution
  task_to_change <- sample(1:length(solution), 1)
  current_server <- solution[task_to_change]
  # Assign to a different server
  possible_servers <- setdiff(1:num_servers, current_server)
  if (length(possible_servers) == 0) {
    # Only one server exists, no change
    return(neighbor)
  }
  new_server <- sample(possible_servers, 1)
  neighbor[task_to_change] <- new_server
  return(neighbor)
}

# Simulated Annealing Algorithm with Objective History Tracking
simulated_annealing <- function(initial_solution, energy_efficiency, execution_time, 
                                cpu_usage, memory_usage, network_traffic, power_consumption, 
                                max_cpu, max_memory, max_net, max_pwr, 
                                num_servers,   # Added parameter
                                max_iter, init_temp, cooling_rate, penalty_weight) {
  # Initialize
  current_solution <- initial_solution
  best_solution <- current_solution
  current_temp <- init_temp
  best_objective <- objective_function(current_solution, energy_efficiency, execution_time, 
                                      cpu_usage, memory_usage, network_traffic, power_consumption, 
                                      max_cpu, max_memory, max_net, max_pwr, penalty_weight)
  
  # Initialize a vector to store the best objective value at each iteration
  objective_history <- numeric(max_iter)
  objective_history[1] <- best_objective
  
  for (i in 2:max_iter) {
    # Generate a neighbor solution
    neighbor <- generate_neighbor(current_solution, num_servers)
    
    # Calculate objective value for the neighbor
    neighbor_objective <- objective_function(neighbor, energy_efficiency, execution_time, 
                                             cpu_usage, memory_usage, network_traffic, power_consumption, 
                                             max_cpu, max_memory, max_net, max_pwr, penalty_weight)
    
    # Calculate the change in objective
    delta_objective <- neighbor_objective - best_objective
    
    # Accept the neighbor if it's better or based on acceptance probability
    if (delta_objective < 0 || runif(1) < exp(-delta_objective / current_temp)) {
      current_solution <- neighbor
      current_objective <- neighbor_objective
      
      # Update the best solution found so far
      if (current_objective < best_objective) {
        best_solution <- current_solution
        best_objective <- current_objective
      }
    }
    
    # Store the best objective so far
    objective_history[i] <- best_objective
    
    # Cool down the temperature
    current_temp <- current_temp * cooling_rate
    
    # Optional: Print progress every 100 iterations
    if (i %% 100 == 0) {
      cat("Iteration:", i, "Best Objective:", best_objective, "\n")
    }
  }
  
  return(list(best_solution = best_solution, best_objective = best_objective, objective_history = objective_history))
}


```


```{r}
# Example usage with Enhanced Parameters

# Sample data (replace this with your actual data)
# Assuming vm_cloud_clean_analysis has columns:
# - energy_efficiency
# - execution_time
# - cpu_usage
# - memory_usage
# - network_traffic
# - power_consumption

energy_efficiency <- vm_cloud_clean_analysis$energy_efficiency
execution_time <- vm_cloud_clean_analysis$execution_time
cpu_usage <- vm_cloud_clean_analysis$cpu_usage
memory_usage <- vm_cloud_clean_analysis$memory_usage
network_traffic <- vm_cloud_clean_analysis$network_traffic
power_consumption <- vm_cloud_clean_analysis$power_consumption

num_tasks <- length(energy_efficiency)
num_servers <- 35  # As per your current setup

# Define server capacities (adjusted as per MILP)
max_cpu <- 80
max_memory <- 80
max_net <- 80
max_pwr <- 80

# Initialize a feasible solution
# Simple heuristic: Assign each task randomly to a server
set.seed(123)  # For reproducibility
initial_solution <- sample(1:num_servers, num_tasks, replace = TRUE)

# Define SA Parameters
max_iter <- 100000
init_temp <- 50
cooling_rate <- 0.995
penalty_weight <- 10  # Penalty for resource overuse; adjust as needed

# Run simulated annealing
result_sa <- simulated_annealing(initial_solution, energy_efficiency, execution_time, 
                                  cpu_usage, memory_usage, network_traffic, power_consumption, 
                                  max_cpu, max_memory, max_net, max_pwr, num_servers,
                                  max_iter, init_temp, cooling_rate, penalty_weight)

# View the best solution and its objective value
print(result_sa$best_solution)
print(paste("Best objective value:", result_sa$best_objective))


```


```{r}
# Plot Objective Function Convergence
library(ggplot2)

# Create a data frame for plotting
objective_df <- data.frame(
  Iteration = 1:max_iter,
  Objective_Value = result_sa$objective_history
)

# Plot using ggplot2
ggplot(objective_df, aes(x = Iteration, y = Objective_Value)) +
  geom_line(color = "blue") +
  labs(title = "Objective Function Convergence Over Iterations",
       x = "Iteration",
       y = "Objective Function Value") +
  theme_minimal()

```


```{r}
# Assuming 'result_sa$best_solution' contains the server assignments for each task

# Create a data frame for task assignments
assignments_sa <- data.frame(
  Task_ID = 1:num_tasks,
  Assigned_Server = result_sa$best_solution
)

# View the first few assignments (optional)
head(assignments_sa)

# Aggregate the number of tasks per server
task_counts <- assignments_sa %>%
  group_by(Assigned_Server) %>%
  summarise(Num_Tasks = n())

# Ensure all servers are represented
task_counts <- task_counts %>%
  complete(Assigned_Server = 1:num_servers, fill = list(Num_Tasks = 0))


# Create a heatmap for Task Assignments
ggplot(task_counts, aes(x = factor(1), y = factor(Assigned_Server), fill = Num_Tasks)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Task Assignments Heatmap",
       x = "",
       y = "Server ID",
       fill = "Number of Tasks") +
  theme_minimal() +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())

# Aggregate the number of tasks per server
task_counts_sa <- assignments_sa %>%
  filter(!is.na(Assigned_Server)) %>%  # Exclude any unassigned tasks, if any
  group_by(Assigned_Server) %>%
  summarise(Num_Tasks = n()) %>%
  complete(Assigned_Server = 1:num_servers, fill = list(Num_Tasks = 0))  # Ensure all servers are represented

# Create a bar plot for Task Distribution in Simulated Annealing
ggplot(task_counts_sa, aes(x = factor(Assigned_Server), y = Num_Tasks)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Number of Tasks Assigned to Each Server",
       x = "Server ID",
       y = "Number of Tasks") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))  # Rotate x-axis labels if needed

```