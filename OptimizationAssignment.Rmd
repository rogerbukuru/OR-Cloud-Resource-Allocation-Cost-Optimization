---
title: "Cloud Resource Allocation and Cost Optimization"
author: "Roger Bukuru"
date: '`r Sys.Date()`'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Cleaning

```{r}
# Load necessary libraries
library(tidyverse)

# Clear workspace
rm(list = ls())

# Load data
fileDir <- "vm_cloud_data.csv"
vm_cloud <- read.csv(fileDir)

# Select relevant columns (skip the first two columns as mentioned)
vm_cloud <- vm_cloud[1:11000, -c(1,2)]

# Step 1: One-hot encode character features
# Identify character columns
char_cols <- vm_cloud %>%
  select(where(is.character))

# One-hot encode using pivot_wider
vm_cloud_encoded <- vm_cloud %>%
  mutate(across(where(is.character), as.factor)) %>%
  model.matrix(~ . - 1, data = .) %>%
  as.data.frame()

# Step 2: Remove rows with null values
vm_cloud_clean <- vm_cloud_encoded %>%
  drop_na()

# Step 3: Min-Max Normalize numeric features
min_max_normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# Apply min-max normalization to numeric columns
vm_cloud_standardized <- vm_cloud_clean %>%
  mutate(across(where(is.numeric), ~ min_max_normalize(.)))

# View the first few rows of the cleaned and normalized data
head(vm_cloud_standardized, 10)


# Select relevant columns for the optimization problem (after one-hot encoding and standardization)
vm_cloud_clean_analysis <- vm_cloud_standardized %>%
  select(cpu_usage, memory_usage, network_traffic, power_consumption,
         num_executed_instructions, execution_time, energy_efficiency, task_priorityhigh, task_prioritylow, task_prioritymedium)

# Preview the cleaned data
head(vm_cloud_clean_analysis)


```

# Linear Programming

```{r}
library(Rglpk)
#vm_cloud_clean_analysis = vm_cloud_clean_analysis[1:1000,]
# Number of tasks and servers
num_tasks <- nrow(vm_cloud_clean_analysis)
num_servers <- 35  # Adjusted number of servers

# Calculate total resource requirements
total_cpu_required <- sum(vm_cloud_clean_analysis$cpu_usage)
total_mem_required <- sum(vm_cloud_clean_analysis$memory_usage)
total_net_required <- sum(vm_cloud_clean_analysis$network_traffic)
total_pwr_required <- sum(vm_cloud_clean_analysis$power_consumption)

cat("Total CPU Required:", total_cpu_required, "\n")
cat("Total Memory Required:", total_mem_required, "\n")
cat("Total Network Traffic Required:", total_net_required, "\n")
cat("Total Power Consumption Required:", total_pwr_required, "\n")

# Server capacities (adjusted based on requirements)
cpu_capacity <- 80
mem_capacity <- 80
net_capacity <- 80
pwr_capacity <- 80

# Calculate total resource capacities
total_cpu_capacity <- cpu_capacity * num_servers
total_mem_capacity <- mem_capacity * num_servers
total_net_capacity <- net_capacity * num_servers
total_pwr_capacity <- pwr_capacity * num_servers

cat("Total CPU Capacity:", total_cpu_capacity, "\n")
cat("Total Memory Capacity:", total_mem_capacity, "\n")
cat("Total Network Traffic Capacity:", total_net_capacity, "\n")
cat("Total Power Consumption Capacity:", total_pwr_capacity, "\n")

# Verify if capacities are sufficient
if (total_cpu_required > total_cpu_capacity ||
    total_mem_required > total_mem_capacity ||
    total_net_required > total_net_capacity ||
    total_pwr_required > total_pwr_capacity) {
  stop("Server capacities are insufficient. Consider increasing the number of servers or their capacities.")
} else {
  cat("Server capacities are sufficient.\n")
}

# Convert to matrix for easier manipulation
tasks_matrix <- as.matrix(vm_cloud_clean_analysis)

# Define a function to perform MILP optimization
perform_milp_optimization <- function(objective_type = "execution_time") {
  if (objective_type == "execution_time") {
    # Minimize Execution Time
    alpha <- 1
    beta <- 0
    obj_vector <- alpha * tasks_matrix[, "execution_time"] - beta * tasks_matrix[, "energy_efficiency"]
  } else if (objective_type == "energy_efficiency") {
    # Maximize Energy Efficiency (minimize negative)
    alpha <- 0
    beta <- 1
    obj_vector <- alpha * tasks_matrix[, "execution_time"] - beta * tasks_matrix[, "energy_efficiency"]
  } else {
    stop("Invalid objective type. Choose 'execution_time' or 'energy_efficiency'.")
  }
  
  # Expand the objective vector for all variables (tasks x servers)
  objective <- rep(0, num_tasks * num_servers)
  
  for (i in 1:num_tasks) {
    for (j in 1:num_servers) {
      index <- (i - 1) * num_servers + j
      objective[index] <- obj_vector[i]
    }
  }
  
  # Define constraints as per your code
  # Task Assignment Constraints
  constraint_matrix_tasks <- matrix(0, nrow = num_tasks, ncol = num_tasks * num_servers)
  
  for (i in 1:num_tasks) {
    for (j in 1:num_servers) {
      index <- (i - 1) * num_servers + j
      constraint_matrix_tasks[i, index] <- 1
    }
  }
  
  constraint_dir_tasks <- rep("==", num_tasks)
  constraint_rhs_tasks <- rep(1, num_tasks)
  
  # CPU Constraints
  constraint_matrix_cpu <- matrix(0, nrow = num_servers, ncol = num_tasks * num_servers)
  for (j in 1:num_servers) {
    for (i in 1:num_tasks) {
      index <- (i - 1) * num_servers + j
      constraint_matrix_cpu[j, index] <- tasks_matrix[i, "cpu_usage"]
    }
  }
  constraint_dir_cpu <- rep("<=", num_servers)
  constraint_rhs_cpu <- rep(cpu_capacity, num_servers)
  
  # Memory Constraints
  constraint_matrix_mem <- matrix(0, nrow = num_servers, ncol = num_tasks * num_servers)
  for (j in 1:num_servers) {
    for (i in 1:num_tasks) {
      index <- (i - 1) * num_servers + j
      constraint_matrix_mem[j, index] <- tasks_matrix[i, "memory_usage"]
    }
  }
  constraint_dir_mem <- rep("<=", num_servers)
  constraint_rhs_mem <- rep(mem_capacity, num_servers)
  
  # Network Traffic Constraints
  constraint_matrix_net <- matrix(0, nrow = num_servers, ncol = num_tasks * num_servers)
  for (j in 1:num_servers) {
    for (i in 1:num_tasks) {
      index <- (i - 1) * num_servers + j
      constraint_matrix_net[j, index] <- tasks_matrix[i, "network_traffic"]
    }
  }
  constraint_dir_net <- rep("<=", num_servers)
  constraint_rhs_net <- rep(net_capacity, num_servers)
  
  # Power Consumption Constraints
  constraint_matrix_pwr <- matrix(0, nrow = num_servers, ncol = num_tasks * num_servers)
  for (j in 1:num_servers) {
    for (i in 1:num_tasks) {
      index <- (i - 1) * num_servers + j
      constraint_matrix_pwr[j, index] <- tasks_matrix[i, "power_consumption"]
    }
  }
  constraint_dir_pwr <- rep("<=", num_servers)
  constraint_rhs_pwr <- rep(pwr_capacity, num_servers)
  
  # Combine all constraints
  constraint_matrix <- rbind(
    constraint_matrix_tasks,
    constraint_matrix_cpu,
    constraint_matrix_mem,
    constraint_matrix_net,
    constraint_matrix_pwr
  )
  
  constraint_dir <- c(
    constraint_dir_tasks,
    constraint_dir_cpu,
    constraint_dir_mem,
    constraint_dir_net,
    constraint_dir_pwr
  )
  
  constraint_rhs <- c(
    constraint_rhs_tasks,
    constraint_rhs_cpu,
    constraint_rhs_mem,
    constraint_rhs_net,
    constraint_rhs_pwr
  )
  
  # Define variable types (all binary)
  var_types <- rep("B", num_tasks * num_servers)
  
  # Define the MILP model
  milp_model <- list(
    obj = objective,
    mat = constraint_matrix,
    dir = constraint_dir,
    rhs = constraint_rhs
  )
  
  # Solve the MILP model using Rglpk
  solution <- Rglpk_solve_LP(
    obj = milp_model$obj,
    mat = milp_model$mat,
    dir = milp_model$dir,
    rhs = milp_model$rhs,
    types = var_types,
    max = FALSE  # Since we're minimizing
  )
  
  return(solution)
}


```



```{r}
# Perform optimization to minimize Execution Time
solution_time <- perform_milp_optimization(objective_type = "execution_time")

# Check the status for Execution Time optimization
if (solution_time$status == 0) {
  cat("Optimal solution for Execution Time minimization found.\n")
} else {
  cat("No optimal solution for Execution Time minimization found. Status code:", solution_time$status, "\n")
}

# Interpret Execution Time Optimization Solution
if (solution_time$status == 0 || solution_time$status == 1) {
  # Extract the solution vector
  solution_vector_time <- solution_time$solution
  
  # Initialize an assignment matrix
  assignment_matrix_time <- matrix(0, nrow = num_tasks, ncol = num_servers)
  
  for (i in 1:num_tasks) {
    for (j in 1:num_servers) {
      index <- (i - 1) * num_servers + j
      assignment_matrix_time[i, j] <- solution_vector_time[index]
    }
  }
  
  # Verify assignments
  row_sums_time <- rowSums(assignment_matrix_time)
  if (all(row_sums_time == 1)) {
    cat("All tasks are assigned to exactly one server for Execution Time minimization.\n")
  } else {
    cat("Some tasks are not properly assigned in Execution Time minimization.\n")
    num_unassigned_time <- sum(row_sums_time == 0)
    cat("Number of unassigned tasks:", num_unassigned_time, "\n")
  }
  
  # Create a data frame for assignments
  assignments_time <- data.frame(
    Task_ID = 1:num_tasks,
    Assigned_Server = apply(assignment_matrix_time, 1, function(x) which(x == 1))
  )
  
  # Assign NA to unassigned tasks
  assignments_time$Assigned_Server[row_sums_time != 1] <- NA
  
  # View first few assignments
  head(assignments_time)
  
  # Extract and print the objective function value
  objective_value_time <- solution_time$optimum
  cat("Objective Function Value (Execution Time Minimization):", objective_value_time, "\n")
  
  
  # Calculate total execution time and energy efficiency
  total_execution_time_time <- sum(tasks_matrix[, "execution_time"] * row_sums_time)
  total_energy_efficiency_time <- sum(tasks_matrix[, "energy_efficiency"] * row_sums_time)
  
  cat("Total Execution Time (Minimized):", total_execution_time_time, "\n")
  cat("Total Energy Efficiency (Minimized):", total_energy_efficiency_time, "\n")
}

```
```{r}
# Load necessary library
library(ggplot2)

# Aggregate the number of tasks per server
task_counts_time <- assignments_time %>%
  filter(!is.na(Assigned_Server)) %>%  # Exclude unassigned tasks
  group_by(Assigned_Server) %>%
  summarise(Num_Tasks = n()) %>%
  complete(Assigned_Server = 1:num_servers, fill = list(Num_Tasks = 0))

# Plot Task Assignment Heatmap
ggplot(task_counts_time, aes(x = factor(1), y = factor(Assigned_Server), fill = Num_Tasks)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Task Assignments Heatmap (MILP - Execution Time Minimization)",
       x = "",
       y = "Server ID",
       fill = "Number of Tasks") +
  theme_minimal() +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())

# Bar Plot for Task Distribution
ggplot(task_counts_time, aes(x = factor(Assigned_Server), y = Num_Tasks)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Number of Tasks Assigned to Each Server (MILP - Execution Time Minimization)",
       x = "Server ID",
       y = "Number of Tasks") +
  theme_minimal()

```



```{r}
# Perform optimization to maximize Energy Efficiency (minimize -Energy Efficiency)
solution_energy <- perform_milp_optimization(objective_type = "energy_efficiency")

# Check the status for Energy Efficiency optimization
if (solution_energy$status == 0) {
  cat("Optimal solution for Energy Efficiency maximization found.\n")
} else {
  cat("No optimal solution for Energy Efficiency maximization found. Status code:", solution_energy$status, "\n")
}

# Interpret Energy Efficiency Optimization Solution
if (solution_energy$status == 0 || solution_energy$status == 1) {
  # Extract the solution vector
  solution_vector_energy <- solution_energy$solution
  
  # Initialize an assignment matrix
  assignment_matrix_energy <- matrix(0, nrow = num_tasks, ncol = num_servers)
  
  for (i in 1:num_tasks) {
    for (j in 1:num_servers) {
      index <- (i - 1) * num_servers + j
      assignment_matrix_energy[i, j] <- solution_vector_energy[index]
    }
  }
  
  # Verify assignments
  row_sums_energy <- rowSums(assignment_matrix_energy)
  if (all(row_sums_energy == 1)) {
    cat("All tasks are assigned to exactly one server for Energy Efficiency maximization.\n")
  } else {
    cat("Some tasks are not properly assigned in Energy Efficiency maximization.\n")
    num_unassigned_energy <- sum(row_sums_energy == 0)
    cat("Number of unassigned tasks:", num_unassigned_energy, "\n")
  }
  
  # Create a data frame for assignments
  assignments_energy <- data.frame(
    Task_ID = 1:num_tasks,
    Assigned_Server = apply(assignment_matrix_energy, 1, function(x) which(x == 1))
  )
  
  # Assign NA to unassigned tasks
  assignments_energy$Assigned_Server[row_sums_energy != 1] <- NA
  
  # View first few assignments
  head(assignments_energy)
  
    # Extract and print the objective function value
  objective_value_time <- solution_time$optimum
  cat("Objective Function Value (Execution Time Minimization):", objective_value_time, "\n")
  
  # Calculate total execution time and energy efficiency
  total_execution_time_energy <- sum(tasks_matrix[, "execution_time"] * row_sums_energy)
  total_energy_efficiency_energy <- sum(tasks_matrix[, "energy_efficiency"] * row_sums_energy)
  
  cat("Total Execution Time (Minimized):", total_execution_time_energy, "\n")
  cat("Total Energy Efficiency (Maximized):", total_energy_efficiency_energy, "\n")
}
```
```{r}
library(ggplot2)

# Aggregate the number of tasks per server
task_counts_time <- assignments_time %>%
  filter(!is.na(Assigned_Server)) %>%  # Exclude unassigned tasks
  group_by(Assigned_Server) %>%
  summarise(Num_Tasks = n()) %>%
  complete(Assigned_Server = 1:num_servers, fill = list(Num_Tasks = 0))

# Plot Task Assignment Heatmap
ggplot(task_counts_time, aes(x = factor(1), y = factor(Assigned_Server), fill = Num_Tasks)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Task Assignments Heatmap (MILP - Energy Efficiency Maximization)",
       x = "",
       y = "Server ID",
       fill = "Number of Tasks") +
  theme_minimal() +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())

# Bar Plot for Task Distribution
ggplot(task_counts_time, aes(x = factor(Assigned_Server), y = Num_Tasks)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Number of Tasks Assigned to Each Server (MILP - Energy Efficiency Maximization)",
       x = "Server ID",
       y = "Number of Tasks") +
  theme_minimal()

```

# Simulated Annealing

```{r}
# Load necessary libraries
library(tidyverse)

# Define the objective function
objective_function <- function(solution, weight_energy, weight_time, energy_efficiency, execution_time) {
  # Objective function is a weighted sum of energy efficiency and execution time
  energy_part <- sum(solution * energy_efficiency)
  time_part <- sum(solution * execution_time)
  return(weight_energy * energy_part + weight_time * time_part)
}

# Generate a random neighbor (small change to the solution)
generate_neighbor <- function(solution) {
  # Flip a random bit (reassign a random task)
  neighbor <- solution
  task_to_change <- sample(1:length(solution), 1)
  neighbor[task_to_change] <- 1 - neighbor[task_to_change]  # Flip 0 to 1 or 1 to 0
  return(neighbor)
}

# Check if the neighbor is feasible based on resource constraints
check_feasibility <- function(solution, cpu_usage, memory_usage, max_cpu, max_memory) {
  total_cpu <- sum(solution * cpu_usage)
  total_memory <- sum(solution * memory_usage)
  return(total_cpu <= max_cpu && total_memory <= max_memory)
}

# Simulated Annealing Algorithm
simulated_annealing <- function(initial_solution, weight_energy, weight_time, energy_efficiency, execution_time,
                                cpu_usage, memory_usage, max_cpu, max_memory, max_iter, init_temp, cooling_rate) {
  # Initialize
  current_solution <- initial_solution
  best_solution <- current_solution
  current_temp <- init_temp
  best_objective <- objective_function(current_solution, weight_energy, weight_time, energy_efficiency, execution_time)
  
  for (i in 1:max_iter) {
    # Generate a neighbor solution
    neighbor <- generate_neighbor(current_solution)
    
    # Check if the neighbor is feasible
    if (check_feasibility(neighbor, cpu_usage, memory_usage, max_cpu, max_memory)) {
      # Calculate objective value for the neighbor
      neighbor_objective <- objective_function(neighbor, weight_energy, weight_time, energy_efficiency, execution_time)
      
      # Calculate the change in objective
      delta_objective <- neighbor_objective - best_objective
      
      # Accept the neighbor if it's better or based on acceptance probability
      if (delta_objective < 0 || runif(1) < exp(-delta_objective / current_temp)) {
        current_solution <- neighbor
        best_objective <- neighbor_objective
        
        # Update the best solution found so far
        if (best_objective < objective_function(best_solution, weight_energy, weight_time, energy_efficiency, execution_time)) {
          best_solution <- current_solution
        }
      }
    }
    
    # Cool down the temperature
    current_temp <- current_temp * cooling_rate
  }
  
  return(list(best_solution = best_solution, best_objective = best_objective))
}

# Example usage

# Sample data (you should replace this with your actual data)
energy_efficiency <- vm_cloud_clean_analysis$energy_efficiency
execution_time <- vm_cloud_clean_analysis$execution_time
cpu_usage <- vm_cloud_clean_analysis$cpu_usage
memory_usage <- vm_cloud_clean_analysis$memory_usage

# Parameters
weight_energy <- 0.5
weight_time <- 0.5
max_cpu <- 0.8  # Max CPU usage allowed
max_memory <- 0.8  # Max memory usage allowed
initial_solution <- sample(c(0, 1), length(energy_efficiency), replace = TRUE)  # Random initial solution
max_iter <- 1000
init_temp <- 10
cooling_rate <- 0.99

# Run simulated annealing
result <- simulated_annealing(initial_solution, weight_energy, weight_time, energy_efficiency, execution_time,
                              cpu_usage, memory_usage, max_cpu, max_memory, max_iter, init_temp, cooling_rate)

# View the best solution and its objective value
print(result$best_solution)
print(paste("Best objective value:", result$best_objective))

which(result$best_solution==1)

# Review temperature reduction method
```

# Genetic Algorithm.

## Objective Function

```{r}
# 1. Objective Function
objective_function <- function(solution, weight_energy, weight_time, energy_efficiency, execution_time) {
  energy_part <- sum(solution * energy_efficiency)
  time_part <- sum(solution * execution_time)
  return(weight_energy * energy_part + weight_time * time_part)
}
```

## Initial Population

```{r}
# 2. Initial Population Generation
generate_initial_population <- function(pop_size, num_tasks) {
  population <- replicate(pop_size, sample(c(0, 1), num_tasks, replace = TRUE))
  return(as.data.frame(t(population)))
}
```


## Selection

```{r}
# 3. Selection: Tournament Selection
tournament_selection <- function(population, fitness, tournament_size) {
  selected_indices <- sample(1:nrow(population), tournament_size, replace = FALSE)
  tournament_fitness <- fitness[selected_indices]
  winner <- selected_indices[which.min(tournament_fitness)]  # Minimizing the objective
  return(population[winner, ])
}

```


## Crossover 

```{r}

# 4. Crossover Operators

## a. Position-Based (n-point) Crossover
n_point_crossover <- function(parent1, parent2, n_points = 2) {
  points <- sort(sample(1:(length(parent1)-1), n_points))
  child1 <- parent1
  child2 <- parent2
  flip <- FALSE
  current_point <- 1
  for (point in points) {
    if (flip) {
      child1[current_point:point] <- parent2[current_point:point]
      child2[current_point:point] <- parent1[current_point:point]
    }
    flip <- !flip
    current_point <- point + 1
  }
  # Handle the segment after the last point
  if (flip) {
    child1[current_point:length(parent1)] <- parent2[current_point:length(parent1)]
    child2[current_point:length(parent2)] <- parent1[current_point:length(parent2)]
  }
  return(list(child1 = child1, child2 = child2))
}

## b. Single-Point Crossover
single_point_crossover <- function(parent1, parent2) {
  point <- sample(1:(length(parent1)-1), 1)
  child1 <- c(parent1[1:point], parent2[(point + 1):length(parent2)])
  child2 <- c(parent2[1:point], parent1[(point + 1):length(parent1)])
  return(list(child1 = child1, child2 = child2))
}

## c. Uniform Crossover
uniform_crossover <- function(parent1, parent2, swap_prob = 0.5) {
  mask <- runif(length(parent1)) < swap_prob
  child1 <- parent1
  child2 <- parent2
  child1[mask] <- parent2[mask]
  child2[mask] <- parent1[mask]
  return(list(child1 = child1, child2 = child2))
}

## d. Order-Based Crossover (Less Applicable for Binary)
order_based_crossover <- function(parent1, parent2) {
  size <- length(parent1)
  child1 <- rep(NA, size)
  child2 <- rep(NA, size)
  
  start <- sample(1:size, 1)
  end <- sample(start:size, 1)
  
  child1[start:end] <- parent1[start:end]
  child2[start:end] <- parent2[start:end]
  
  fill_order <- function(child, parent_other) {
    current_pos <- 1
    for (gene in parent_other) {
      if (!(gene %in% child)) {
        while (!is.na(child[current_pos])) {
          current_pos <- current_pos + 1
        }
        child[current_pos] <- gene
      }
    }
    # Replace remaining NAs with 0 (since binary)
    child[is.na(child)] <- 0
    return(child)
  }
  
  child1 <- fill_order(child1, parent2)
  child2 <- fill_order(child2, parent1)
  
  return(list(child1 = child1, child2 = child2))
}

## e. Partially Mapped Crossover (PMX) (Less Applicable for Binary)
pmx_crossover <- function(parent1, parent2) {
  size <- length(parent1)
  points <- sort(sample(1:size, 2))
  start <- points[1]
  end <- points[2]
  
  child1 <- parent1
  child2 <- parent2
  
  # Mapping segments
  mapping1 <- parent1[start:end]
  mapping2 <- parent2[start:end]
  
  # Swap the mapped segment
  child1[start:end] <- mapping2
  child2[start:end] <- mapping1
  
  return(list(child1 = child1, child2 = child2))
}
```

## Mutation

```{r}
# 5. Mutation Operators

## a. Random Swap Mutation
random_swap_mutation <- function(individual, mutation_rate = 0.01) {
  for (i in 1:length(individual)) {
    if (runif(1) < mutation_rate) {
      individual[i] <- 1 - individual[i]
    }
  }
  return(individual)
}

## b. Inversion Mutation
inversion_mutation <- function(individual, mutation_rate = 0.01) {
  for (i in 1:length(individual)) {
    if (runif(1) < mutation_rate) {
      individual[i] <- 1 - individual[i]
    }
  }
  return(individual)
}

## c. Scramble Mutation
scramble_mutation <- function(individual, mutation_rate = 0.05) {
  if (runif(1) < mutation_rate) {
    # Define the subset to scramble
    subset_size <- sample(2:5, 1)
    if (length(individual) < subset_size) {
      subset_size <- length(individual)
    }
    start <- sample(1:(length(individual) - subset_size + 1), 1)
    end <- start + subset_size - 1
    subset <- individual[start:end]
    scrambled_subset <- sample(subset)
    individual[start:end] <- scrambled_subset
  }
  return(individual)
}

## d. Insert Mutation
insert_mutation <- function(individual, mutation_rate = 0.05) {
  if (runif(1) < mutation_rate) {
    pos1 <- sample(1:length(individual), 1)
    pos2 <- sample(1:length(individual), 1)
    while (pos2 == pos1) {
      pos2 <- sample(1:length(individual), 1)
    }
    value <- individual[pos1]
    individual <- individual[-pos1]
    individual <- append(individual, value, after = pos2 - 1)
  }
  return(individual)
}


```

## Feasibility Check

```{r}
# 6. Feasibility Check
check_feasibility <- function(solution, cpu_usage, memory_usage, max_cpu, max_memory) {
  total_cpu <- sum(solution * cpu_usage)
  total_memory <- sum(solution * memory_usage)
  return(total_cpu <= max_cpu && total_memory <= max_memory)
}

```


```{r}
# 7. Generation Models

## a. Generational Model
# Handled within the main GA loop

## b. Steady-State Model
# Handled within the main GA loop

# 8. Genetic Algorithm Function
genetic_algorithm <- function(
  energy_efficiency,
  execution_time,
  cpu_usage,
  memory_usage,
  weight_energy = 0.5,
  weight_time = 0.5,
  max_cpu = 0.8,
  max_memory = 0.8,
  pop_size = 100,
  num_generations = 100,
  crossover_rate = 0.8,
  mutation_rate = 0.01,
  tournament_size = 3,
  crossover_type = "n_point",  # Options: "n_point", "single_point", "uniform", "order_based", "pmx"
  mutation_type = "swap",       # Options: "swap", "inversion", "scramble", "insert"
  generation_model = "generational"  # Options: "generational", "steady_state"
) {
  num_tasks <- length(energy_efficiency)
  
  # Initialize population
  population <- generate_initial_population(pop_size, num_tasks)
  
  # Evaluate fitness
  fitness <- apply(population, 1, function(sol) {
    obj <- objective_function(as.numeric(sol), weight_energy, weight_time, energy_efficiency, execution_time)
    return(obj)
  })
  
  # Initialize best solution tracking
  best_fitness_history <- numeric(num_generations)
  best_solution <- population[which.min(fitness), ]
  best_fitness <- min(fitness)
  
  for (gen in 1:num_generations) {
    new_population <- population
    
    if (generation_model == "generational") {
      # Generational Model: Create a new population from scratch
      offspring <- list()
      while (length(offspring) < pop_size) {
        # Selection
        parent1 <- tournament_selection(population, fitness, tournament_size)
        parent2 <- tournament_selection(population, fitness, tournament_size)
        
        # Crossover
        if (runif(1) < crossover_rate) {
          if (crossover_type == "n_point") {
            children <- n_point_crossover(as.numeric(parent1), as.numeric(parent2), n_points = 2)
          } else if (crossover_type == "single_point") {
            children <- single_point_crossover(as.numeric(parent1), as.numeric(parent2))
          } else if (crossover_type == "uniform") {
            children <- uniform_crossover(as.numeric(parent1), as.numeric(parent2), swap_prob = 0.5)
          } else if (crossover_type == "order_based") {
            children <- order_based_crossover(as.numeric(parent1), as.numeric(parent2))
          } else if (crossover_type == "pmx") {
            children <- pmx_crossover(as.numeric(parent1), as.numeric(parent2))
          } else {
            # Default to no crossover
            children <- list(child1 = as.numeric(parent1), child2 = as.numeric(parent2))
          }
        } else {
          children <- list(child1 = as.numeric(parent1), child2 = as.numeric(parent2))
        }
        
        # Mutation
        if (mutation_type == "swap") {
          children$child1 <- random_swap_mutation(children$child1, mutation_rate)
          children$child2 <- random_swap_mutation(children$child2, mutation_rate)
        } else if (mutation_type == "inversion") {
          children$child1 <- inversion_mutation(children$child1, mutation_rate)
          children$child2 <- inversion_mutation(children$child2, mutation_rate)
        } else if (mutation_type == "scramble") {
          children$child1 <- scramble_mutation(children$child1, mutation_rate)
          children$child2 <- scramble_mutation(children$child2, mutation_rate)
        } else if (mutation_type == "insert") {
          children$child1 <- insert_mutation(children$child1, mutation_rate)
          children$child2 <- insert_mutation(children$child2, mutation_rate)
        }
        
        # Check feasibility and add to offspring
        if (check_feasibility(children$child1, cpu_usage, memory_usage, max_cpu, max_memory)) {
          offspring <- append(offspring, list(children$child1))
        }
        if (length(offspring) < pop_size && check_feasibility(children$child2, cpu_usage, memory_usage, max_cpu, max_memory)) {
          offspring <- append(offspring, list(children$child2))
        }
      }
      
      # Convert offspring list to data frame
      new_population <- as.data.frame(do.call(rbind, offspring[1:pop_size]))
      
    } else if (generation_model == "steady_state") {
      # Steady-State Model: Replace a few individuals at a time
      num_offsprings <- 2
      for (i in 1:(pop_size / 2)) {
        # Selection
        parent1 <- tournament_selection(population, fitness, tournament_size)
        parent2 <- tournament_selection(population, fitness, tournament_size)
        
        # Crossover
        if (runif(1) < crossover_rate) {
          if (crossover_type == "n_point") {
            children <- n_point_crossover(as.numeric(parent1), as.numeric(parent2), n_points = 2)
          } else if (crossover_type == "single_point") {
            children <- single_point_crossover(as.numeric(parent1), as.numeric(parent2))
          } else if (crossover_type == "uniform") {
            children <- uniform_crossover(as.numeric(parent1), as.numeric(parent2), swap_prob = 0.5)
          } else if (crossover_type == "order_based") {
            children <- order_based_crossover(as.numeric(parent1), as.numeric(parent2))
          } else if (crossover_type == "pmx") {
            children <- pmx_crossover(as.numeric(parent1), as.numeric(parent2))
          } else {
            # Default to no crossover
            children <- list(child1 = as.numeric(parent1), child2 = as.numeric(parent2))
          }
        } else {
          children <- list(child1 = as.numeric(parent1), child2 = as.numeric(parent2))
        }
        
        # Mutation
        if (mutation_type == "swap") {
          children$child1 <- random_swap_mutation(children$child1, mutation_rate)
          children$child2 <- random_swap_mutation(children$child2, mutation_rate)
        } else if (mutation_type == "inversion") {
          children$child1 <- inversion_mutation(children$child1, mutation_rate)
          children$child2 <- inversion_mutation(children$child2, mutation_rate)
        } else if (mutation_type == "scramble") {
          children$child1 <- scramble_mutation(children$child1, mutation_rate)
          children$child2 <- scramble_mutation(children$child2, mutation_rate)
        } else if (mutation_type == "insert") {
          children$child1 <- insert_mutation(children$child1, mutation_rate)
          children$child2 <- insert_mutation(children$child2, mutation_rate)
        }
        
        # Replace the worst individuals with the new children
        for (child in list(children$child1, children$child2)) {
          if (check_feasibility(child, cpu_usage, memory_usage, max_cpu, max_memory)) {
            child_obj <- objective_function(child, weight_energy, weight_time, energy_efficiency, execution_time)
            # Find the worst individual
            worst_index <- which.max(fitness)
            if (child_obj < fitness[worst_index]) {
              population[worst_index, ] <- child
              fitness[worst_index] <- child_obj
              # Update best solution
              if (child_obj < best_fitness) {
                best_fitness <- child_obj
                best_solution <- population[worst_index, ]
              }
            }
          }
        }
      }
      new_population <- population
    }
    
    # Evaluate fitness of the new population
    fitness <- apply(new_population, 1, function(sol) {
      obj <- objective_function(as.numeric(sol), weight_energy, weight_time, energy_efficiency, execution_time)
      return(obj)
    })
    
    # Update best solution
    current_best_fitness <- min(fitness)
    current_best_solution <- new_population[which.min(fitness), ]
    if (current_best_fitness < best_fitness) {
      best_fitness <- current_best_fitness
      best_solution <- current_best_solution
    }
    
    best_fitness_history[gen] <- best_fitness
    
    # Update population
    population <- new_population
  }
  
  return(list(
    best_solution = best_solution,
    best_fitness = best_fitness,
    fitness_history = best_fitness_history
  ))
}
```


```{r}
# Example usage

energy_efficiency <- vm_cloud_clean_analysis$energy_efficiency
execution_time <- vm_cloud_clean_analysis$execution_time
cpu_usage <- vm_cloud_clean_analysis$cpu_usage
memory_usage <- vm_cloud_clean_analysis$memory_usage

# GA Parameters
weight_energy <- 0.5
weight_time <- 0.5
max_cpu <- 0.8  # Max CPU usage allowed
max_memory <- 0.8  # Max memory usage allowed
pop_size <- 100
num_generations <- 2
crossover_rate <- 0.8
mutation_rate <- 0.01
tournament_size <- 3
crossover_type <- "single_point"  # Choose among "n_point", "single_point", "uniform", "order_based", "pmx"
mutation_type <- "swap"            # Choose among "swap", "inversion", "scramble", "insert"
generation_model <- "generational"  # Choose between "generational", "steady_state"

# Run Genetic Algorithm
ga_result <- genetic_algorithm(
  energy_efficiency = energy_efficiency,
  execution_time = execution_time,
  cpu_usage = cpu_usage,
  memory_usage = memory_usage,
  weight_energy = weight_energy,
  weight_time = weight_time,
  max_cpu = max_cpu,
  max_memory = max_memory,
  pop_size = pop_size,
  num_generations = num_generations,
  crossover_rate = crossover_rate,
  mutation_rate = mutation_rate,
  tournament_size = tournament_size,
  crossover_type = crossover_type,
  mutation_type = mutation_type,
  generation_model = generation_model
)

# View the best solution and its objective value
print("Best Solution:")
print(ga_result$best_solution)
print(paste("Best Objective Value:", ga_result$best_fitness))

# Identify selected tasks
selected_tasks <- which(as.numeric(ga_result$best_solution) == 1)
print("Selected Tasks:")
print(selected_tasks)

# Plot fitness over generations
plot(ga_result$fitness_history, type = "l", col = "blue",
     xlab = "Generation", ylab = "Best Fitness",
     main = "Fitness over Generations")

```

# Multi-Objective Goal Programming

## Archemedian 

```{r}
# Archemedian Method Function using Rglpk
archemedian_goal_programming <- function(data, weights, max_cpu, max_memory) {
  # Decision variables: x_i = 1 if task i is selected, 0 otherwise
  # Objective: Minimize w1 * sum(energy_efficiency_i * x_i) + w2 * sum(execution_time_i * x_i)
  
  # Objective coefficients
  obj_coeff <- weights["energy"] * data$energy_efficiency + 
               weights["time"] * data$execution_time
  
  # Constraint coefficients
  cpu_constraint <- data$cpu_usage
  memory_constraint <- data$memory_usage
  
  # Constraint matrix
  constr_matrix <- rbind(cpu_constraint, memory_constraint)
  
  # Constraint directions
  constr_dir <- c("<=", "<=")
  
  # Constraint right-hand side
  constr_rhs <- c(max_cpu, max_memory)
  
  # Define bounds for variables (binary: 0 or 1)
  bounds <- list(lower = list(ind = 1:num_tasks, val = rep(0, num_tasks)),
                 upper = list(ind = 1:num_tasks, val = rep(1, num_tasks)))
  
  # Define variable types (binary)
  var_types <- rep("B", num_tasks)
  
  # Solve the MIP
  solution <- Rglpk_solve_LP(
    obj = obj_coeff,
    mat = constr_matrix,
    dir = constr_dir,
    rhs = constr_rhs,
    bounds = bounds,
    types = var_types,
    max = FALSE
  )
  
  if (solution$status == 0) {  # 0 indicates success
    selected_tasks <- which(solution$solution == 1)
    total_energy <- sum(data$energy_efficiency[selected_tasks])
    total_time <- sum(data$execution_time[selected_tasks])
    return(list(
      selected_tasks = selected_tasks,
      total_energy = total_energy,
      total_time = total_time,
      objective_value = solution$objval
    ))
  } else {
    return(NULL)
  }
}

```

## Tchebycheff 

```{r}
# Tchebycheff Method Function using Rglpk
tchebycheff_goal_programming <- function(data, weights, max_cpu, max_memory) {
  # Ideal values (minimum possible values)
  G1_ideal <- min(data$energy_efficiency)
  G2_ideal <- min(data$execution_time)
  
  # Number of decision variables (tasks) and auxiliary variable Z
  num_tasks <- nrow(data)
  
  # Objective: Minimize Z
  # Total variables: x1, x2, ..., xn, Z
  # Objective coefficients: 0 for x_i, 1 for Z
  obj_coeff <- c(rep(0, num_tasks), 1)
  
  # Constraint matrix
  # Each constraint corresponds to a goal's weighted deviation
  # Constraint 1: Z >= w1 * (G1_ideal - sum(G1_i * x_i))
  # Constraint 2: Z >= w2 * (G2_ideal - sum(G2_i * x_i))
  # Constraint 3: sum(CPU_i * x_i) <= Max_CPU
  # Constraint 4: sum(Memory_i * x_i) <= Max_Memory
  
  # Constraint 1 coefficients: -w1 * G1_i for x_i and 1 for Z
  constr1 <- c(-weights["energy"] * data$energy_efficiency, 1)
  
  # Constraint 2 coefficients: -w2 * G2_i for x_i and 1 for Z
  constr2 <- c(-weights["time"] * data$execution_time, 1)
  
  # Constraint 3 coefficients: CPU_i for x_i and 0 for Z
  constr3 <- c(data$cpu_usage, 0)
  
  # Constraint 4 coefficients: Memory_i for x_i and 0 for Z
  constr4 <- c(data$memory_usage, 0)
  
  # Combine constraints into a matrix
  constr_matrix <- rbind(constr1, constr2, constr3, constr4)
  
  # Constraint directions
  constr_dir <- c(">=", ">=", "<=", "<=")
  
  # Constraint right-hand sides
  constr_rhs <- c(
    weights["energy"] * G1_ideal,
    weights["time"] * G2_ideal,
    max_cpu,
    max_memory
  )
  
  # Define bounds for variables
  # x_i: binary (0 or 1)
  # Z: continuous, >=0
  # Rglpk allows defining separate bounds; we'll define for each variable
  # Variable indices: 1 to num_tasks are x_i, num_tasks + 1 is Z
  bounds <- list(
    lower = list(ind = 1:(num_tasks + 1), val = rep(0, num_tasks + 1)),
    upper = list(ind = 1:(num_tasks + 1), val = c(rep(1, num_tasks), Inf))
  )
  
  # Define variable types
  var_types <- c(rep("B", num_tasks), "C")  # "B" for binary, "C" for continuous
  
  # Solve the MIP
  solution <- Rglpk_solve_LP(
    obj = obj_coeff,
    mat = constr_matrix,
    dir = constr_dir,
    rhs = constr_rhs,
    bounds = bounds,
    types = var_types,
    max = FALSE
  )
  
  if (solution$status == 0) {  # 0 indicates success
    x_solution <- solution$solution[1:num_tasks]
    Z_value <- solution$solution[num_tasks + 1]
    
    selected_tasks <- which(x_solution == 1)
    total_energy <- sum(data$energy_efficiency[selected_tasks])
    total_time <- sum(data$execution_time[selected_tasks])
    
    return(list(
      selected_tasks = selected_tasks,
      total_energy = total_energy,
      total_time = total_time,
      Z = Z_value
    ))
  } else {
    return(NULL)
  }
}

```


```{r}
# Define weights for the objectives
# Adjust weights based on the importance of each goal
weights <- c(energy = 0.5, time = 0.5)  # Equal importance

# Define resource constraints
max_cpu <- 1.0   # Maximum CPU usage allowed
max_memory <- 1.0  # Maximum memory usage allowed

# Apply Archemedian Goal Programming
archemedian_result <- archemedian_goal_programming(
  data = vm_cloud_clean_analysis,
  weights = weights,
  max_cpu = max_cpu,
  max_memory = max_memory
)

# Check if a solution was found
if (!is.null(archemedian_result)) {
  cat("Archemedian Method Solution:\n")
  cat("Selected Tasks:", archemedian_result$selected_tasks, "\n")
  cat("Total Energy Consumption:", archemedian_result$total_energy, "\n")
  cat("Total Execution Time:", archemedian_result$total_time, "\n")
  cat("Objective Value:", archemedian_result$objective_value, "\n\n")
} else {
  cat("No feasible solution found using the Archemedian Method.\n\n")
}

# Apply Tchebycheff Goal Programming
tchebycheff_result <- tchebycheff_goal_programming(
  data = vm_cloud_clean_analysis,
  weights = weights,
  max_cpu = max_cpu,
  max_memory = max_memory
)

# Check if a solution was found
if (!is.null(tchebycheff_result)) {
  cat("Tchebycheff Method Solution:\n")
  cat("Selected Tasks:", tchebycheff_result$selected_tasks, "\n")
  cat("Total Energy Consumption:", tchebycheff_result$total_energy, "\n")
  cat("Total Execution Time:", tchebycheff_result$total_time, "\n")
  cat("Z (Maximum Weighted Deviation):", tchebycheff_result$Z, "\n")
} else {
  cat("No feasible solution found using the Tchebycheff Method.\n")
}

```